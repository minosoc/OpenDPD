# Run DPD 단계에서 GRU 모델 입력 데이터 분석

## 데이터 흐름 개요

```
CSV 파일 → numpy 배열 → PyTorch Tensor → GRU 모델
```

## 1. 입력 데이터 `x` 분석

### 1.1 데이터 소스
- **위치**: `steps/run_dpd.py` 27번째 줄
  ```python
  _, _, _, _, X_test, _ = load_dataset(dataset_name=proj.dataset_name)
  ```

### 1.2 원본 데이터 형태
- **파일**: `datasets/DPA_200MHz/test_input.csv`
- **형식**: CSV 파일 (헤더: `I,Q`)
- **내용 예시**:
  ```
  I,Q
  0.020894198,-0.068800244
  0.073526179,-0.036218004
  0.100883919,0.004726009
  ...
  ```

### 1.3 데이터 로딩 과정
1. **CSV → numpy 배열** (`modules/data_collector.py:75`)
   ```python
   X_test = pd.read_csv(path_dataset / 'test_input.csv').to_numpy()
   ```
   - **Shape**: `(N, 2)` 
     - `N`: 시퀀스 길이 (샘플 수, 예: 7681)
     - `2`: I와 Q 두 개의 특징값

2. **numpy → PyTorch Tensor** (`steps/run_dpd.py:76`)
   ```python
   dpd_in = torch.Tensor(X_test).unsqueeze(dim=0).to(proj.device)
   ```
   - `torch.Tensor(X_test)`: numpy 배열을 PyTorch 텐서로 변환
     - Shape: `(N, 2)` → `(N, 2)` (변화 없음)
   - `.unsqueeze(dim=0)`: 배치 차원 추가
     - Shape: `(N, 2)` → `(1, N, 2)`
     - `(batch_size=1, sequence_length=N, features=2)`
   - `.to(proj.device)`: GPU/CPU로 이동

### 1.4 최종 입력 `x` 형태
- **Shape**: `(1, N, 2)`
  - `1`: 배치 크기 (단일 시퀀스)
  - `N`: 시퀀스 길이 (시간 스텝 수, 예: 7681)
  - `2`: 특징 차원 (I, Q)
- **데이터 타입**: `torch.FloatTensor`
- **값 범위**: 실수값 (I/Q 신호의 정규화된 값)

### 1.5 GRU 모델로 전달 (`models.py:158`)
```python
out = self.backbone(x, h_0)
```
- `x`: Shape `(1, N, 2)` - 배치 차원이 첫 번째 (`batch_first=True`)
- GRU는 각 시간 스텝 `t`에서 `(1, 2)` 크기의 입력을 받음

## 2. 초기 은닉 상태 `h_0` 분석

### 2.1 생성 위치
- **위치**: `models.py:150-155`
  ```python
  def forward(self, x, h_0=None):
      device = x.device
      batch_size = x.size(0)  # batch_size = 1

      if h_0 is None:  # Create initial hidden states if necessary
          h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)
  ```

### 2.2 생성 조건
- `h_0`가 `None`일 때 자동 생성
- `run_dpd.py`에서는 `h_0`를 명시적으로 전달하지 않음
  ```python
  dpd_out = net_dpd(dpd_in)  # h_0=None (기본값)
  ```

### 2.3 `h_0` 형태
- **Shape**: `(num_layers, batch_size, hidden_size)`
  - `num_layers`: GRU 레이어 수 (예: 1)
  - `batch_size`: 배치 크기 (예: 1)
  - `hidden_size`: 은닉 상태 차원 (예: 15)
- **예시 Shape**: `(1, 1, 15)` (단일 레이어, 배치 크기 1, 은닉 크기 15)

### 2.4 초기값
- **모든 값이 0으로 초기화**: `torch.zeros(...)`
- **의미**: 시퀀스 시작 시점의 은닉 상태가 모두 0
- **장치**: 입력 `x`와 동일한 디바이스 (GPU/CPU)

### 2.5 GRU 레이어에서의 사용
- PyTorch `nn.GRU`는 `h_0`를 받아서:
  - 첫 번째 레이어의 초기 은닉 상태로 사용
  - 각 시간 스텝마다 은닉 상태가 업데이트됨
  - 마지막 시간 스텝의 은닉 상태는 버려짐 (`out, _ = self.rnn(x, h_0)`)

## 3. 전체 데이터 흐름 다이어그램

```
┌─────────────────────────────────────────────────────────────┐
│ 1. CSV 파일 로딩                                             │
│    test_input.csv → pandas DataFrame                        │
│    Shape: (N, 2) - [I, Q] 값들                             │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│ 2. numpy 배열 변환                                          │
│    pd.read_csv(...).to_numpy()                              │
│    Shape: (N, 2) - numpy.ndarray                            │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│ 3. PyTorch Tensor 변환                                      │
│    torch.Tensor(X_test)                                     │
│    Shape: (N, 2) - torch.Tensor                             │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│ 4. 배치 차원 추가                                            │
│    .unsqueeze(dim=0)                                        │
│    Shape: (1, N, 2) - (batch, sequence, features)           │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│ 5. 디바이스 이동                                            │
│    .to(proj.device)                                         │
│    GPU 또는 CPU로 이동                                       │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│ 6. CoreModel.forward()                                      │
│    - x: (1, N, 2)                                           │
│    - h_0 생성: torch.zeros(num_layers, 1, hidden_size)      │
│      Shape: (1, 1, 15) - 모든 값이 0                        │
└─────────────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────────────┐
│ 7. GRU.forward(x, h_0)                                      │
│    - 입력 x: (1, N, 2)                                      │
│    - 초기 은닉 상태 h_0: (1, 1, 15)                         │
│    - 각 시간 스텝 t에서:                                     │
│      * 입력: x[:, t, :] → (1, 2)                            │
│      * 은닉 상태 업데이트                                    │
│    - 출력: (1, N, hidden_size) → (1, N, 2)                  │
└─────────────────────────────────────────────────────────────┘
```

## 4. 실제 예시 값

### 4.1 입력 데이터 예시
```python
# X_test (numpy, 첫 3개 샘플)
array([[ 0.020894198, -0.068800244],
       [ 0.073526179, -0.036218004],
       [ 0.100883919,  0.004726009],
       ...])

# dpd_in (PyTorch Tensor, 배치 차원 추가 후)
tensor([[[ 0.0209, -0.0688],
         [ 0.0735, -0.0362],
         [ 0.1009,  0.0047],
         ...]])
# Shape: (1, 7681, 2)
```

### 4.2 초기 은닉 상태 예시
```python
# h_0 (num_layers=1, batch_size=1, hidden_size=15)
tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])
# Shape: (1, 1, 15)
```

## 5. 주요 특징

1. **시퀀스 처리**: 전체 테스트 시퀀스를 한 번에 처리 (배치 크기 1)
2. **I/Q 신호**: 복소 신호를 I(실부)와 Q(허부)로 분리하여 표현
3. **제로 초기화**: 은닉 상태는 0으로 시작하여 시퀀스를 따라 학습된 상태로 업데이트
4. **배치 처리**: 단일 시퀀스지만 배치 차원을 유지하여 모델 호환성 보장

